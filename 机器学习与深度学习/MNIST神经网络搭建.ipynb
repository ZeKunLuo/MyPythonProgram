{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to Mnist_data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Mnist_data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to Mnist_data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Mnist_data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to Mnist_data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Mnist_data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to Mnist_data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Mnist_data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "batch_size = 200\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "datasets.MNIST('Mnist_data',train = True,download = True,\n",
    "                transform = transforms.Compose([\n",
    "                    #  将数据变换为张量形式\n",
    "                    transforms.ToTensor(),\n",
    "                    # 根据官方提供的均值和标准差,标准化数据方便减少梯度下降次数\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))                  \n",
    "                ])),batch_size = batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "datasets.MNIST('Mnist_data',train = False,download = True,\n",
    "              transform = transforms.Compose([\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "              ])),batch_size = batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: Mnist_data\n",
      "    Split: Train\n",
      "(0, [tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]), tensor([2, 1, 4, 4, 3, 7, 4, 1, 0, 0, 1, 9, 7, 9, 7, 9, 7, 4, 8, 9, 7, 3, 6, 7,\n",
      "        9, 9, 7, 1, 9, 0, 5, 3, 4, 7, 1, 5, 5, 8, 8, 9, 2, 4, 7, 8, 1, 9, 3, 4,\n",
      "        8, 9, 3, 5, 1, 2, 9, 3, 7, 5, 7, 8, 1, 3, 4, 2, 0, 0, 4, 9, 7, 3, 8, 6,\n",
      "        1, 5, 2, 1, 1, 9, 7, 4, 7, 5, 3, 6, 5, 8, 6, 4, 8, 5, 2, 9, 6, 4, 6, 8,\n",
      "        4, 2, 5, 1, 6, 3, 7, 7, 8, 7, 7, 0, 5, 7, 0, 9, 2, 5, 0, 0, 0, 8, 5, 3,\n",
      "        8, 7, 8, 7, 2, 9, 1, 7, 9, 9, 4, 7, 6, 6, 7, 0, 6, 3, 6, 0, 6, 6, 9, 2,\n",
      "        2, 8, 8, 0, 9, 7, 1, 6, 1, 1, 8, 3, 2, 7, 4, 7, 9, 8, 3, 0, 3, 9, 1, 5,\n",
      "        3, 2, 8, 2, 5, 9, 7, 1, 5, 0, 8, 5, 6, 2, 3, 8, 6, 8, 7, 8, 2, 0, 3, 3,\n",
      "        3, 4, 7, 7, 2, 4, 3, 8])])\n"
     ]
    }
   ],
   "source": [
    "print(type(train_loader))\n",
    "print(train_loader.dataset)\n",
    "#查看数据结构\n",
    "for i in enumerate(train_loader):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0 [0/60000(0.0%)]\t loss:2.306537389755249\n",
      "Train epoch 0 [20000/60000(33.33%)]\t loss:2.035033702850342\n",
      "Train epoch 0 [40000/60000(66.67%)]\t loss:1.2957611083984375\n",
      "Train epoch 0 [60000/60000(100.00%)]\t loss:0.7836695909500122\n",
      "epoch:0  test_data Average_loss : 0.003627978050708771\t Accuracy: 8302.0/10000(83.020000)%\n",
      "Train epoch 1 [0/60000(0.0%)]\t loss:0.7361741065979004\n",
      "Train epoch 1 [20000/60000(33.33%)]\t loss:0.49886584281921387\n",
      "Train epoch 1 [40000/60000(66.67%)]\t loss:0.49449071288108826\n",
      "Train epoch 1 [60000/60000(100.00%)]\t loss:0.3523230254650116\n",
      "epoch:1  test_data Average_loss : 0.002026969975233078\t Accuracy: 8896.0/10000(88.960000)%\n",
      "Train epoch 2 [0/60000(0.0%)]\t loss:0.4014418125152588\n",
      "Train epoch 2 [20000/60000(33.33%)]\t loss:0.3810940682888031\n",
      "Train epoch 2 [40000/60000(66.67%)]\t loss:0.44071638584136963\n",
      "Train epoch 2 [60000/60000(100.00%)]\t loss:0.3752475082874298\n",
      "epoch:2  test_data Average_loss : 0.0016781002402305603\t Accuracy: 9025.0/10000(90.250000)%\n",
      "Train epoch 3 [0/60000(0.0%)]\t loss:0.35575544834136963\n",
      "Train epoch 3 [20000/60000(33.33%)]\t loss:0.35479432344436646\n",
      "Train epoch 3 [40000/60000(66.67%)]\t loss:0.37836602330207825\n",
      "Train epoch 3 [60000/60000(100.00%)]\t loss:0.34169018268585205\n",
      "epoch:3  test_data Average_loss : 0.0015032748386263847\t Accuracy: 9110.0/10000(91.100000)%\n",
      "Train epoch 4 [0/60000(0.0%)]\t loss:0.2739598751068115\n",
      "Train epoch 4 [20000/60000(33.33%)]\t loss:0.38402724266052246\n",
      "Train epoch 4 [40000/60000(66.67%)]\t loss:0.30347561836242676\n",
      "Train epoch 4 [60000/60000(100.00%)]\t loss:0.2502043843269348\n",
      "epoch:4  test_data Average_loss : 0.0014004800230264663\t Accuracy: 9194.0/10000(91.940000)%\n",
      "Train epoch 5 [0/60000(0.0%)]\t loss:0.24062280356884003\n",
      "Train epoch 5 [20000/60000(33.33%)]\t loss:0.31439653038978577\n",
      "Train epoch 5 [40000/60000(66.67%)]\t loss:0.2230578362941742\n",
      "Train epoch 5 [60000/60000(100.00%)]\t loss:0.21400272846221924\n",
      "epoch:5  test_data Average_loss : 0.001314392538368702\t Accuracy: 9229.0/10000(92.290000)%\n",
      "Train epoch 6 [0/60000(0.0%)]\t loss:0.25108906626701355\n",
      "Train epoch 6 [20000/60000(33.33%)]\t loss:0.3200721740722656\n",
      "Train epoch 6 [40000/60000(66.67%)]\t loss:0.2467733919620514\n",
      "Train epoch 6 [60000/60000(100.00%)]\t loss:0.26089826226234436\n",
      "epoch:6  test_data Average_loss : 0.0012480686396360396\t Accuracy: 9290.0/10000(92.900000)%\n",
      "Train epoch 7 [0/60000(0.0%)]\t loss:0.2835906147956848\n",
      "Train epoch 7 [20000/60000(33.33%)]\t loss:0.2755715250968933\n",
      "Train epoch 7 [40000/60000(66.67%)]\t loss:0.2002713531255722\n",
      "Train epoch 7 [60000/60000(100.00%)]\t loss:0.20591193437576294\n",
      "epoch:7  test_data Average_loss : 0.0011820814564824104\t Accuracy: 9322.0/10000(93.220000)%\n",
      "Train epoch 8 [0/60000(0.0%)]\t loss:0.31660526990890503\n",
      "Train epoch 8 [20000/60000(33.33%)]\t loss:0.202341690659523\n",
      "Train epoch 8 [40000/60000(66.67%)]\t loss:0.2645059823989868\n",
      "Train epoch 8 [60000/60000(100.00%)]\t loss:0.21705631911754608\n",
      "epoch:8  test_data Average_loss : 0.00112670366615057\t Accuracy: 9345.0/10000(93.450000)%\n",
      "Train epoch 9 [0/60000(0.0%)]\t loss:0.2496471405029297\n",
      "Train epoch 9 [20000/60000(33.33%)]\t loss:0.18550094962120056\n",
      "Train epoch 9 [40000/60000(66.67%)]\t loss:0.2601899802684784\n",
      "Train epoch 9 [60000/60000(100.00%)]\t loss:0.2624129354953766\n",
      "epoch:9  test_data Average_loss : 0.0010824317336082458\t Accuracy: 9382.0/10000(93.820000)%\n"
     ]
    }
   ],
   "source": [
    "class First_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(First_nn,self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28,200),\n",
    "            #  因为数据被标准化了可能为负值为防止梯度为零用LeakyReLU\n",
    "            nn.LeakyReLU(inplace = True),\n",
    "            nn.Linear(200,200),\n",
    "            nn.LeakyReLU(inplace = True),\n",
    "            nn.Linear(200,10),\n",
    "            nn.LeakyReLU(inplace = True)\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "# 将模型搬到GPU上\n",
    "device = torch.device('cuda:0')\n",
    "#net = First_nn().cuda()\n",
    "net = First_nn().to(device)\n",
    "#随机梯度下降\n",
    "#Error:optimizer = optim.SGD(net.parameters(),lr = learning_rate).to(device)\n",
    "optimizer = optim.SGD(net.parameters(),lr = learning_rate)\n",
    "# softmax激活 + CrossEntropy损失\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data = data.view(data.size(0),1*28*28)\n",
    "        data = data.to(device)\n",
    "        target = target.cuda()\n",
    "        logits = net(data)\n",
    "        loss = loss_func(logits,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx+1)%100 ==0:\n",
    "            print('Train epoch {} [{}/{}({:.2f}%)]\\t loss:{}'.format(\n",
    "                    epoch,(batch_idx+1)*len(data),len(train_loader.dataset),\n",
    "                    100.*(batch_idx+1)*len(data)/len(train_loader.dataset),loss.item()))\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print('Train epoch {} [{}/{}({}%)]\\t loss:{}'.format(\n",
    "                   epoch,batch_idx*len(data),len(train_loader.dataset),\n",
    "                   batch_idx/len(train_loader.dataset),loss.item()))\n",
    "    # test module\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx,(data,target) in enumerate(test_loader):\n",
    "        data = data.view(data.size(0),28*28)\n",
    "        data = data.to(device)\n",
    "        target = target.cuda()\n",
    "        logits = net(data)\n",
    "        test_loss += loss_func(logits,target).item()\n",
    "        predict = logits.argmax(dim=1)\n",
    "        correct += predict.eq(target).float().sum().item() \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('epoch:{}  test_data Average_loss : {}\\t Accuracy: {}/{}({:2f})%'.format(\n",
    "              epoch,test_loss,correct,len(test_loader.dataset),100.*correct/len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bca0e2660b9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
