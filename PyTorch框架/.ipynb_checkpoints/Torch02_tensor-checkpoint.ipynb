{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引、切片、连接、变异操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.cat(seq,dim=0,out=None)->Tensor`\n",
    "在给定维度上对输入张量序列seq进行连接操作由这可以想到卷积神经网络\n",
    "的全连接层\n",
    "`torch.cat()`可以看做`torch.split()`和`torch.chunk()`的逆运算\n",
    "* seq:任意(Tensors的序列)\n",
    "* dim:张量连接尺寸\n",
    "* out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2403, -0.5606,  0.0291],\n",
      "        [ 0.2671,  0.7454,  1.9962]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(2,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2403, -0.5606,  0.0291],\n",
      "        [ 0.2671,  0.7454,  1.9962],\n",
      "        [ 0.2403, -0.5606,  0.0291],\n",
      "        [ 0.2671,  0.7454,  1.9962]])\n",
      "tensor([[ 0.2403, -0.5606,  0.0291,  0.2403, -0.5606,  0.0291],\n",
      "        [ 0.2671,  0.7454,  1.9962,  0.2671,  0.7454,  1.9962]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.cat((x,x),dim=0)\n",
    "print(y)\n",
    "z = torch.cat((x,x),dim=1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3847,  0.9418, -0.8916],\n",
      "         [ 0.5488,  1.0235,  0.2930],\n",
      "         [-0.2274,  0.2622,  1.5223]],\n",
      "\n",
      "        [[-1.7501, -0.2777, -1.1390],\n",
      "         [ 1.6078, -0.9990, -1.3961],\n",
      "         [ 1.4809,  1.1763, -0.3197]],\n",
      "\n",
      "        [[-0.4526, -0.1629,  0.0279],\n",
      "         [-0.9971, -0.3301,  0.1106],\n",
      "         [ 1.4824,  0.2740, -0.2242]]])\n",
      "tensor([[[-0.3847,  0.9418, -0.8916, -0.3847,  0.9418, -0.8916],\n",
      "         [ 0.5488,  1.0235,  0.2930,  0.5488,  1.0235,  0.2930],\n",
      "         [-0.2274,  0.2622,  1.5223, -0.2274,  0.2622,  1.5223]],\n",
      "\n",
      "        [[-1.7501, -0.2777, -1.1390, -1.7501, -0.2777, -1.1390],\n",
      "         [ 1.6078, -0.9990, -1.3961,  1.6078, -0.9990, -1.3961],\n",
      "         [ 1.4809,  1.1763, -0.3197,  1.4809,  1.1763, -0.3197]],\n",
      "\n",
      "        [[-0.4526, -0.1629,  0.0279, -0.4526, -0.1629,  0.0279],\n",
      "         [-0.9971, -0.3301,  0.1106, -0.9971, -0.3301,  0.1106],\n",
      "         [ 1.4824,  0.2740, -0.2242,  1.4824,  0.2740, -0.2242]]])\n",
      "tensor([[[-0.3847,  0.9418, -0.8916],\n",
      "         [ 0.5488,  1.0235,  0.2930],\n",
      "         [-0.2274,  0.2622,  1.5223],\n",
      "         [-0.3847,  0.9418, -0.8916],\n",
      "         [ 0.5488,  1.0235,  0.2930],\n",
      "         [-0.2274,  0.2622,  1.5223]],\n",
      "\n",
      "        [[-1.7501, -0.2777, -1.1390],\n",
      "         [ 1.6078, -0.9990, -1.3961],\n",
      "         [ 1.4809,  1.1763, -0.3197],\n",
      "         [-1.7501, -0.2777, -1.1390],\n",
      "         [ 1.6078, -0.9990, -1.3961],\n",
      "         [ 1.4809,  1.1763, -0.3197]],\n",
      "\n",
      "        [[-0.4526, -0.1629,  0.0279],\n",
      "         [-0.9971, -0.3301,  0.1106],\n",
      "         [ 1.4824,  0.2740, -0.2242],\n",
      "         [-0.4526, -0.1629,  0.0279],\n",
      "         [-0.9971, -0.3301,  0.1106],\n",
      "         [ 1.4824,  0.2740, -0.2242]]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(3,3,3)\n",
    "print(x1)\n",
    "y1 = torch.cat((x1,x1),dim=2)\n",
    "z1 = torch.cat((x1,x1),dim=1)\n",
    "print(y1)\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.chunk(tensor,chunks,dim)->>tensors`\n",
    "将张量沿给定维度分块\n",
    "* chunks:分块数\n",
    "* dim:指定分块维度\n",
    "我们可以利用chunk和cat,先chunk后cat就能将张量\n",
    "向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.0026, -0.4883,  0.7212]]), tensor([[-0.8090,  0.5345, -0.4566]]), tensor([[ 0.5391, -1.5702,  0.2347]]))\n",
      "(tensor([[ 0.0026],\n",
      "        [-0.8090],\n",
      "        [ 0.5391]]), tensor([[-0.4883],\n",
      "        [ 0.5345],\n",
      "        [-1.5702]]), tensor([[ 0.7212],\n",
      "        [-0.4566],\n",
      "        [ 0.2347]]))\n",
      "tensor([[ 0.0026, -0.4883,  0.7212, -0.8090,  0.5345, -0.4566,  0.5391, -1.5702,\n",
      "          0.2347]])\n",
      "tensor([[ 0.0026],\n",
      "        [-0.8090],\n",
      "        [ 0.5391],\n",
      "        [-0.4883],\n",
      "        [ 0.5345],\n",
      "        [-1.5702],\n",
      "        [ 0.7212],\n",
      "        [-0.4566],\n",
      "        [ 0.2347]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,3)\n",
    "a = torch.chunk(x,3,dim=0)\n",
    "b = torch.chunk(x,3,dim=1)\n",
    "print(a)\n",
    "print(b)\n",
    "print(torch.cat((a[0],a[1],a[2]),dim=1)) # 按列连接\n",
    "print(torch.cat((b[0],b[1],b[2]),dim=0)) # 按行连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.gather(input,dim,index,out=None,sparse_grad=Flase,out=None)->Tensor`\n",
    "\n",
    "通俗点解释就是把指定索引dim的下标进行替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 2.]])\n",
      "tensor([[1., 1.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.LongTensor([[0,0],[1,2]])\n",
    "t = torch.Tensor([[1,2],[3,4]])\n",
    "# 将4个元素的第一个维度下标替换成0,0,1,0\n",
    "print(torch.gather(t,0,torch.tensor([[0,0],[1,0]])))\n",
    "# 将4个元素的第二个维度下标替换成0,0,0,1\n",
    "print(torch.gather(t,1,torch.tensor([[0,0],[0,1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.index_select(input,dim,index,out=None)->>Tensor`\n",
    "* input:输入张量\n",
    "* dim(int):索引维度\n",
    "* index(LongTensor):索引下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9431,  0.3714,  2.0795,  0.3076],\n",
      "        [-1.3413, -0.2518,  0.0665, -0.7127],\n",
      "        [ 0.1430,  0.3192,  0.2244, -3.1105]])\n",
      "tensor([[-1.3413, -0.2518,  0.0665, -0.7127],\n",
      "        [ 0.1430,  0.3192,  0.2244, -3.1105]])\n",
      "tensor([[ 0.3714,  2.0795,  0.3076],\n",
      "        [-0.2518,  0.0665, -0.7127],\n",
      "        [ 0.3192,  0.2244, -3.1105]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,4)\n",
    "print(x)\n",
    "# 按行索引,2,3行\n",
    "print(torch.index_select(x,0,torch.LongTensor([1,2])))\n",
    "# 按列索引,2,3,4列\n",
    "print(torch.index_select(x,1,torch.LongTensor([1,2,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.masked_select(input,mask,out=None)->>Tensor`\n",
    "根据mask输出一个一维张量\n",
    "* input:输入张量\n",
    "* mask(ByteTensor):模板只含有0,1二值张量必须与input张量维度一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6960, -1.2918],\n",
      "        [ 0.4685,  2.1284]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:67: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6960, 2.1284])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.ByteTensor([[1,0],[0,1]])\n",
    "y = torch.randn(2,2)\n",
    "print(y)\n",
    "print(torch.masked_select(y,mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.split(tensor,split_size,dim=0)->>tensor`\n",
    "如果可分,张量沿着指定维度指定大小进行分割,直到大小不足则停止\n",
    "* tensor\n",
    "ch* split_size:分割尺寸\n",
    "* dim(int):分割维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1178,  0.8798, -0.2490,  1.3292],\n",
      "        [-0.6898, -0.7655,  1.2808, -0.3745],\n",
      "        [ 0.4639,  1.9754,  0.7621, -2.3465]])\n",
      "(tensor([[ 1.1178],\n",
      "        [-0.6898],\n",
      "        [ 0.4639]]), tensor([[ 0.8798],\n",
      "        [-0.7655],\n",
      "        [ 1.9754]]), tensor([[-0.2490],\n",
      "        [ 1.2808],\n",
      "        [ 0.7621]]), tensor([[ 1.3292],\n",
      "        [-0.3745],\n",
      "        [-2.3465]]))\n",
      "(tensor([[ 1.1178,  0.8798, -0.2490,  1.3292],\n",
      "        [-0.6898, -0.7655,  1.2808, -0.3745]]), tensor([[ 0.4639,  1.9754,  0.7621, -2.3465]]))\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(3,4)\n",
    "print(z)\n",
    "print(torch.split(z,1,dim=1))\n",
    "print(torch.split(z,2,dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.t(input,out=None)->Tensor`\n",
    "张量转置相当于\n",
    "`torch.transpose(input,o,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3948,  0.6652,  0.1606,  0.6889],\n",
      "        [ 0.3723,  0.5568,  1.2067,  0.3230],\n",
      "        [-0.5423,  0.8979, -0.3635, -0.7440]])\n",
      "tensor([[-0.3948,  0.3723, -0.5423],\n",
      "        [ 0.6652,  0.5568,  0.8979],\n",
      "        [ 0.1606,  1.2067, -0.3635],\n",
      "        [ 0.6889,  0.3230, -0.7440]])\n",
      "tensor([[-0.3948,  0.3723, -0.5423],\n",
      "        [ 0.6652,  0.5568,  0.8979],\n",
      "        [ 0.1606,  1.2067, -0.3635],\n",
      "        [ 0.6889,  0.3230, -0.7440]])\n",
      "tensor([[-0.3948,  0.3723, -0.5423],\n",
      "        [ 0.6652,  0.5568,  0.8979],\n",
      "        [ 0.1606,  1.2067, -0.3635],\n",
      "        [ 0.6889,  0.3230, -0.7440]])\n",
      "tensor([[-0.3948,  0.6652,  0.1606,  0.6889],\n",
      "        [ 0.3723,  0.5568,  1.2067,  0.3230],\n",
      "        [-0.5423,  0.8979, -0.3635, -0.7440]])\n"
     ]
    }
   ],
   "source": [
    "n = torch.randn(3,4)\n",
    "print(n)\n",
    "print(torch.t(n))\n",
    "print(torch.transpose(n,0,1))\n",
    "print(torch.transpose(n,1,0))\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机抽样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173688027046900\n",
      "<torch._C.Generator object at 0x0000022D14609090>\n"
     ]
    }
   ],
   "source": [
    "#返回原始种子\n",
    "print(torch.initial_seed())\n",
    "#设置种子返回一个生成器对象\n",
    "print(torch.manual_seed(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.bernoulli(input,out=None)`\n",
    "\n",
    "从伯努利分布中抽取二元随机数(0或者1)这里的bernoulli概率p是随机的\n",
    "输入张量值需是一个概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9744, 0.3189, 0.2148],\n",
      "        [0.9263, 0.4735, 0.5949],\n",
      "        [0.7956, 0.7635, 0.2137]])\n",
      "tensor([[1., 1., 0.],\n",
      "        [1., 0., 1.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(3,3).uniform_(0,1)\n",
    "print(a)\n",
    "print(torch.bernoulli(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.multinomial(input,num_samples,replacement=Flase,out=None)->>LongTensor`\n",
    "从输入张量中每行取num_samples个样本,可以设置replacement设置是否重复取值\n",
    "返回取值的下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3050, 0.8421, 0.9032],\n",
      "        [0.6319, 0.6535, 0.8703],\n",
      "        [0.9382, 0.1838, 0.2943]])\n",
      "tensor([[1, 0],\n",
      "        [2, 1],\n",
      "        [0, 1]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(3,3)\n",
    "d = torch.multinomial(c,2)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.normal(means,std,out)->>tensor`\n",
    "按照指定均值和方差选取样本,均值个数决定样本个数\n",
    "若均值和方差都为张量则两个张量元素个数必须相等\n",
    "* means(Tensor):均值\n",
    "* std(Tensor):方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9111,  0.4022, -0.4876,  0.9415,  0.4344, -0.0191,  0.6611,  0.5365,\n",
       "         0.3165,  0.4264])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean=0.5,std=torch.arange(1,0,-0.1))\n",
    "#torch.normal(mean=torch.arange(1.0, 11.0), std=torch.arange(1, 0, -0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math operations\n",
    "`torch.abs(input,out)->tensor`  输出张量元素绝对值\n",
    "\n",
    "`torch.acos(input,out)`   求反余弦\n",
    "\n",
    "`torch.add(input,value,out)`    对每个张量元素逐个加上value\n",
    "\n",
    "`torch.addcdiv(tensor,value=1,tensor1,tensor2)` 张量(tensor1/tensor2)*value+tensor\n",
    "\n",
    "`torch.addmul`  相乘相加\n",
    "\n",
    "`torch.ceil(input,out)` 向上取整\n",
    "\n",
    "`torch.clamp(input,min,max,out=None)`\n",
    "将元素调整至[min,max]区间\n",
    "\n",
    "`torch.div(input,value)` 除\n",
    "\n",
    "`torch.exp(tensor,out)`  指数\n",
    "\n",
    "`torch.floor(input,out)` 向下去整\n",
    "\n",
    "`torch.fmod(input,divisor,out)` 取余数\n",
    "\n",
    "`torch.frac`  取分数部分\n",
    "\n",
    "`torch.lerp(start, end, weight, out=None)` \n",
    "线性插值:out = start+weight*(end-start)\n",
    "\n",
    "`torch.log` 取自然对数\n",
    "\n",
    "`torch.mul(input, value, out=None)`\n",
    "\n",
    "`torch.mul(input, other, out=None)`  哈达玛积\n",
    "\n",
    "`torch.neg`  取复数\n",
    "\n",
    "`torch.pow(input, exponent, out=None)` 求幂\n",
    "\n",
    "`torch.reciprocal(input, out=None) → Tensor` 去倒数\n",
    "\n",
    "`torch.remainder(input, divisor, out=None) → Tensor` 取余数\n",
    "\n",
    "`torch.rsqrt(input, out=None) → Tensor` 平方根倒数\n",
    "\n",
    "`torch.sigmoid(input, out=None) → Tensor` sigmoid值\n",
    "\n",
    "`torch.sigmoid(input, out=None) → Tensor` 符号函数\n",
    "\n",
    "`torch.cumprod(input, dim, out=None) → Tensor` 按指定维度累积\n",
    "\n",
    "`torch.cumsum(input, dim, out=None) → Tensor`  指定维度累加\n",
    "\n",
    "`torch.dist(input, other, p=2, out=None) → Tensor` 求P范数\n",
    "\n",
    "`torch.mean(input) → float`  均值\n",
    "\n",
    "`torch.mean(input, dim, out=None) → Tensor`  指定维度均值\n",
    "\n",
    "`torch.median(input, dim=-1, values=None, indices=None) -> (Tensor, LongTensor)`  指定维度中位数\n",
    "\n",
    "`torch.mode(input, dim=-1, values=None, indices=None) -> (Tensor, LongTensor)\n",
    "` 众数\n",
    "\n",
    "`torch.norm(input, p, dim, out=None) → Tensor` 指定维度p范数\n",
    "\n",
    "`torch.prod(input) → float` 所有积\n",
    "\n",
    "`torch.prod(input, dim, out=None) → Tensor`  指定维度积\n",
    "\n",
    "`torch.std(input, dim, out=None) → Tensor` 标准差\n",
    "\n",
    "`torch.sum(input, dim, out=None) → Tensor` 按维度求和\n",
    "\n",
    "`torch.sum(input) → float` 所有元素和\n",
    "\n",
    "`var` 按行方差，所有元素方差\n",
    "\n",
    "`torch.eq(input, other, out=None) → Tensor` 相等比较操作 返回01\n",
    "\n",
    "`torch.equal(tensor1, tensor2) → bool`  张量比较shape and value返回bool\n",
    "\n",
    "`torch.ge(input, other, out=None) → Tensor`  大于\n",
    "\n",
    "`torch.gt(input, other, out=None) → Tensor` 与equal类似返回不同\n",
    "\n",
    "`torch.kthvalue(input, k, dim=None, out=None) -> (Tensor, LongTensor)`  取指定维度最小值\n",
    "\n",
    "`torch.le(input, other, out=None) → Tensor` 小于等于\n",
    "\n",
    "`torch.lt(input, other, out=None) → Tensor` 小于\n",
    "\n",
    "`torch.max(input, dim, max=None, max_indices=None) -> (Tensor, LongTensor)`  返回指定维度最大值和索引\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "tensor([1., 2., 3., 4., 5., 5., 5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(1,10,10)\n",
    "print(x)\n",
    "print(torch.clamp(x,1,5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
