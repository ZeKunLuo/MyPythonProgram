{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class torcn.nn.Parameter()`\n",
    "`Parameters`是`Variable`的子类。\n",
    "参数说明:\n",
    "* data(Tensor)->parameter tensor\n",
    "* requires_grad(bool,optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class torch.nn.Module`\n",
    "所有神经网络模块的基类\n",
    "你的模型也应该继承这个类\n",
    "`Modules`还可以包含其他模块,允许将它们嵌套在树结构中.您可以将子模块分配为常规属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入nn包\n",
    "import torch.nn as nn\n",
    "#导入nn包的函数模块\n",
    "import torch.nn.functional as F\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        #调用父类构造方法\n",
    "        super(Model,self).__init__()\n",
    "        #super(nn.Module,self).__init__()\n",
    "        #nn.Module.__init__()\n",
    "        #调用父类中的卷积方法\n",
    "        self.conv1 = nn.Conv2d(1,20,5)\n",
    "        self.conv2 = nn.Conv2d(20,20,5)\n",
    "    def forward(self,x):\n",
    "        # 前向传播使用relu激活函数\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 以这种方式分配的子模块将被注册,并且在调用`.cuda()`时候会转换GPU参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        #super(Model,self).__init__()\n",
    "        nn.Module.__init__(self)\n",
    "        self.add_module(\"conv\",nn.Conv2d(10,20,4))\n",
    "        #self.conv = nn.Conv2d(10,20,4)和上面增加module的方式等价\n",
    "model = Model()\n",
    "print(model.conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CLASS torch.nn.Linear(in_feature,out_feature,bias=True)`\n",
    "此为线性回归公式,也为DNN网络的输出公式,或全连接层的连接公式\n",
    "$Y = XA^{T}+b $\n",
    "参数解释:\n",
    "* in_feature:假设输入尺寸为[rowsize,columsize]则其为columsize\n",
    "* out_feature:输出尺寸大小或者下一层神经元的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2078, -0.3180],\n",
      "        [ 0.1222,  0.5744]], requires_grad=True)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "L = nn.Linear(2,2)\n",
    "print(L.weight)\n",
    "print(L.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 20])\n",
      "torch.Size([128, 30])\n",
      "torch.Size([30])\n",
      "tensor([[-0.9075,  0.4658, -0.6769,  ...,  0.8770,  0.1963, -0.1634],\n",
      "        [ 0.3249, -0.4842,  0.5415,  ..., -0.6922,  0.3830, -0.4454],\n",
      "        [ 0.2934, -0.3098,  0.8307,  ..., -0.2225,  0.1665, -0.7599],\n",
      "        ...,\n",
      "        [ 1.3914, -0.4900,  1.7351,  ..., -0.4768,  0.1307, -2.4975],\n",
      "        [-0.3248,  0.1314,  0.1383,  ..., -0.5789, -0.3377, -1.0821],\n",
      "        [ 1.0866,  0.2304,  1.0595,  ..., -0.0817, -0.9459, -0.2057]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(128, 20)  # 输入的维度是（128，20）\n",
    "m = torch.nn.Linear(20, 30)  # 20,30是指维度\n",
    "c = m(x)\n",
    "print(m.weight.shape)\n",
    "print(c.shape)\n",
    "print(m.bias.shape)\n",
    "#print(c.bias.shape)\n",
    "#print(m.shape)\n",
    "print(c)\n",
    "#语法错误n = torch.nn.Linear(20,torch.tensor([20,20]))\n",
    "#print(n.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply(function)`\n",
    "\n",
    "递归调用函数应用到每个组成部分以及本身,典型应用为初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    print(m)\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(1.0)\n",
    "        print(m.weight)\n",
    "# Sequential中的单元按顺序执行\n",
    "net = nn.Sequential(nn.Linear(2,2),nn.Linear(2,2))\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`modules` \n",
    "\n",
    "返回所有模块的迭代器包括自身重复的模块只迭代一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "1 -> Linear(in_features=2, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "l = nn.Linear(2,2)\n",
    "net = nn.Sequential(l,l)\n",
    "#enumerate进行dict处理\n",
    "for index,m in enumerate(net.modules()):\n",
    "    print(index,'->',m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`named_children()->SonModuleIteration`\n",
    "\n",
    "返回子模块迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "<generator object Module.named_children at 0x000001ABCFDCD2C8>\n"
     ]
    }
   ],
   "source": [
    "for name,module in model.named_children():\n",
    "    #if name in['conv2','conv2']:\n",
    "        print(module)\n",
    "print(model.named_children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`named_modules()`\n",
    "\n",
    "返回网络中所有模块迭代器,同时产生模块的名称以及模块本身\n",
    "重复的模块只返回一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> ('', Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "))\n",
      "1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n"
     ]
    }
   ],
   "source": [
    "l = nn.Linear(2,2)\n",
    "net = nn.Sequential(l,l)\n",
    "for index,m in enumerate(net.named_modules()):\n",
    "    print(index,'->',m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`named_parameters(memo=None,prefix)`\n",
    "\n",
    "返回模块参数的迭代器,同时产生参数的名称和参数本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight -> Parameter containing:\n",
      "tensor([[ 0.3842,  0.3218],\n",
      "        [ 0.1494, -0.4355]], requires_grad=True)\n",
      "bias -> Parameter containing:\n",
      "tensor([0.3653, 0.6474], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,param in l.named_parameters():\n",
    "    print(name,'->',param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parameters()->ModuleParamIteration`\n",
    "\n",
    "这通常被传递个优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([2, 2])\n",
      "<class 'torch.Tensor'> torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for param in l.parameters():\n",
    "    print(type(param.data),param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`register_backward_hook(hook)`\n",
    "\n",
    "在模块上注册一个向后的钩子这是用于记录反向传播时的梯度的\n",
    "每当计算相对于模块输入的梯度时,将调用该钩子挂钩应\n",
    "具有一下签名:\n",
    "\n",
    "`hook(module,grad_input,grad_output)->Variable or None`\n",
    "```python\n",
    "|-如果module如果module有多个输入输出的话，那么grad_input grad_output将会是个tuple。 hook不应该修改它的arguments，但是它可以选择性的返回关于输入的梯度，这个返回的梯度在后续的计算中会替代grad_input。\n",
    "这个函数返回一个句柄(handle)。它有一个方法 handle.remove()，可以用这个方法将hook从module移除\n",
    "```\n",
    "`register_forward_hook(hook)`\n",
    "\n",
    "在模块上注册一个forward hook,这是记录前向传播时的梯度的\n",
    "\n",
    "`hook(module,grad_input,grad_output)->Variable or None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class torch.nn.Sequential(* args)`\n",
    "\n",
    "这是一个时序容器.`Modules`会以他们传入的顺序添加到容器中当然也可以传入\n",
    "一个顺序字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Conv2d(1,20,5),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(20,64,5),\n",
    "                      nn.ReLU()\n",
    "                     )\n",
    "# 传入一个容器\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class torch.nn.ModuleList(modules=None)`\n",
    "\n",
    "将`submodules`保存在一个列表中\n",
    "* modules(list,optional):要添加的模块列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = self.linears[i // 2](x) + l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`append(module)`:追加一个给定模块\n",
    "\n",
    "`extend(modules)`:追加一个模块列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class torch.nn.ParameterList(parameters=None)`\n",
    "\n",
    "在列表中保存参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterList(\n",
      "    (0): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "    (1): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "    (2): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "    (3): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "    (4): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "    (5): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "    (6): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "    (7): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "    (8): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "    (9): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n",
    "        print(self.params)\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for i, p in enumerate(self.params):\n",
    "            x = self.params[i // 2].mm(x) + p.mm(x)\n",
    "        return x\n",
    "    \n",
    "Lmodule = MyModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)`\n",
    "\n",
    "二维卷积:\n",
    "输入形式`input[ batch_size, channels, height_1, width_1 ]`\n",
    "* batch_size 一个batch中样例的个数    \n",
    "* channels 通道数，也就是当前层的深度 \n",
    "* height_1, 图片的高                                 \n",
    "* width_1, 图片的宽  \n",
    "`Conv2d参数:`\n",
    "* in_channels:输入通道数\n",
    "* out_channels:输出feature map数即核的数量\n",
    "* kerner_size: 核的尺寸大小可以是元组\n",
    "* stride:卷积移动步长\n",
    "* padding: 填充数目\n",
    "* dilation: 相当于卷积上采样填充零dilation相当于卷积元素之间隔的0个数\n",
    "* groups:\n",
    "```python\n",
    "通道分组比如说6个in_channels和6个out_channels(6个核设每个核的大小为3x3)分成两组记为in1,in2(都为三个通道),out1,out2(都为三个通道),(in1,out1)为一组,(in2,out2)为一组,这时候每个out1,out2的核都分别处理in1,in2的三个通道,也就是说out1,out2中的每个核的通道数为3,(这时候的参数为6个核x每个核对应的三个通道滤波器x每个滤波器对应的尺寸(长x宽) =  6x3x3x3这比远啦分成1组的时候6x6x3x3减少了一半的参数,分组的前提是,\n",
    "in_channels must be divisible by groups \n",
    "out_channels must be divisible by groups\n",
    "```\n",
    "* bias:有无偏置量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in_channels must be divisible by groups",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-ce31f9a7d10b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)\u001b[0m\n\u001b[0;32m    325\u001b[0m         super(Conv2d, self).__init__(\n\u001b[0;32m    326\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m             False, _pair(0), groups, bias, padding_mode)\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0min_channels\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'in_channels must be divisible by groups'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout_channels\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'out_channels must be divisible by groups'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in_channels must be divisible by groups"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=1, groups=3)\n",
    "conv.weight.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "out_channels must be divisible by groups",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-57b2a67b159f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)\u001b[0m\n\u001b[0;32m    325\u001b[0m         super(Conv2d, self).__init__(\n\u001b[0;32m    326\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m             False, _pair(0), groups, bias, padding_mode)\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'in_channels must be divisible by groups'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout_channels\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'out_channels must be divisible by groups'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: out_channels must be divisible by groups"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=3, groups=2)\n",
    "conv.weight.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 3, 3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=3, groups=2)\n",
    "conv.weight.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 3, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=3, groups=2)\n",
    "conv.weight.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 3, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=3, groups=3)\n",
    "conv.weight.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
