{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`\n",
    "\n",
    "# 参数:#\n",
    "* kernel_size(int or tuple) - max pooling的窗口大小\n",
    "* stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size\n",
    "* padding(int or tuple, optional) - 输入的每一条边补充0的层数\n",
    "* dilation(int or tuple, optional) – mask元素间的距离\n",
    "* return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助\n",
    "* ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作\n",
    "\n",
    "# input:#\n",
    "* input(batch_size,channels,H_in,W_in)\n",
    "# output: #\n",
    "* output(batch_size,channels,H_out,W_out)\n",
    "也就是说输入输出都是个4维张量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上池化:\n",
    "\n",
    "这个主要用于误差反向传播从池化层到卷积层,如果是Rule核池化层到卷积层的误差传播就是在原有误差的基础上上池化\n",
    "\n",
    "`class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)`\n",
    "### 参数: ###\n",
    "* kernel_size(int or tuple) - max pooling的窗口大小\n",
    "* stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size\n",
    "* padding(int or tuple, optional) - 输入的每一条边补充0的层数\n",
    "\n",
    "### 输入参数: ###\n",
    "\n",
    "`input:`张量tensor,包含(batch_size,channels,H_in,W_in)\n",
    "\n",
    "`indices:` Maxpool1d的索引号\n",
    "\n",
    "`output_size:`一个指定输出大小的`torch.Size`\n",
    "\n",
    "### 输出: ###\n",
    "\n",
    "`output`:(batch_size,channels,H_out,W_out)\n",
    "\n",
    "也就是说池化层上采样输入与输出的张量中都包含了批次样本数目,通道数,输出的尺寸\n",
    "\n",
    "其实DNN网络除了全连接层都包含了批次样本数目,通道数目,样本的维数(这里除掉通道),三维池化层输入的是个5维张量,输出的也是个5维张量,以此类推"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模拟二维池化和上池化\n",
      "[[[[7 8 0]\n",
      "   [6 8 2]\n",
      "   [8 7 7]]\n",
      "\n",
      "  [[0 2 7]\n",
      "   [9 9 3]\n",
      "   [1 0 7]]\n",
      "\n",
      "  [[2 1 1]\n",
      "   [0 5 9]\n",
      "   [0 3 8]]\n",
      "\n",
      "  [[7 5 6]\n",
      "   [6 5 8]\n",
      "   [6 1 4]]]]\n",
      "tensor([[[[7., 8., 0.],\n",
      "          [6., 8., 2.],\n",
      "          [8., 7., 7.]],\n",
      "\n",
      "         [[0., 2., 7.],\n",
      "          [9., 9., 3.],\n",
      "          [1., 0., 7.]],\n",
      "\n",
      "         [[2., 1., 1.],\n",
      "          [0., 5., 9.],\n",
      "          [0., 3., 8.]],\n",
      "\n",
      "         [[7., 5., 6.],\n",
      "          [6., 5., 8.],\n",
      "          [6., 1., 4.]]]], dtype=torch.float32)\n",
      "查看池化后元组\n",
      "(tensor([[[[8., 8.],\n",
      "          [8., 8.]],\n",
      "\n",
      "         [[9., 9.],\n",
      "          [9., 9.]],\n",
      "\n",
      "         [[5., 9.],\n",
      "          [5., 9.]],\n",
      "\n",
      "         [[7., 8.],\n",
      "          [6., 8.]]]], dtype=torch.float32), tensor([[[[1, 1],\n",
      "          [4, 4]],\n",
      "\n",
      "         [[3, 4],\n",
      "          [3, 4]],\n",
      "\n",
      "         [[4, 5],\n",
      "          [4, 5]],\n",
      "\n",
      "         [[0, 5],\n",
      "          [3, 5]]]]))\n",
      "查看最大池化后数据\n",
      "tensor([[[[4, 4],\n",
      "          [6, 4]],\n",
      "\n",
      "         [[1, 1],\n",
      "          [6, 4]],\n",
      "\n",
      "         [[4, 5],\n",
      "          [4, 5]],\n",
      "\n",
      "         [[3, 5],\n",
      "          [3, 5]]]])\n",
      "tensor([[[[8., 8.],\n",
      "          [8., 8.]],\n",
      "\n",
      "         [[9., 9.],\n",
      "          [9., 9.]],\n",
      "\n",
      "         [[5., 9.],\n",
      "          [5., 9.]],\n",
      "\n",
      "         [[7., 8.],\n",
      "          [6., 8.]]]], dtype=torch.float32)\n",
      "查看池化后索引\n",
      "tensor([[[[1, 1],\n",
      "          [4, 4]],\n",
      "\n",
      "         [[3, 4],\n",
      "          [3, 4]],\n",
      "\n",
      "         [[4, 5],\n",
      "          [4, 5]],\n",
      "\n",
      "         [[0, 5],\n",
      "          [3, 5]]]])\n",
      "在原有最大池化的基础上上池化\n",
      "tensor([[[[0., 8., 0.],\n",
      "          [0., 8., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [9., 9., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 5., 9.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[7., 0., 0.],\n",
      "          [6., 0., 8.],\n",
      "          [0., 0., 0.]]]], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "m = nn.MaxPool2d(2,1,return_indices=True)\n",
    "print('模拟二维池化和上池化')\n",
    "#模拟定义输入图像矩阵\n",
    "r = np.random.randint(10,size=(1,4,3,3))\n",
    "print(r)\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "#numpy数组转张量\n",
    "r = torch.from_numpy(r)\n",
    "#int 转float\n",
    "newr = r.float()\n",
    "print(newr)\n",
    "#OpData,indices = m(newr)\n",
    "print('查看池化后元组')\n",
    "a = m(newr)\n",
    "print(a)\n",
    "print('查看最大池化后数据')\n",
    "#print(OpData)\n",
    "#print('查看池化后索引')\n",
    "print(indices)\n",
    "print(a[0])\n",
    "print('查看池化后索引')\n",
    "print(a[1])\n",
    "print('在原有最大池化的基础上上池化')\n",
    "upsample = nn.MaxUnpool2d(2,1)\n",
    "print(upsample(a[0],a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
